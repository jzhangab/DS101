{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning\n",
    "## Diabetes Classification/Prediction\n",
    "In this notebook you will implement your first machine learning algorithm to analyze a population health dataset: the Pima Indian diabetes dataset. The purpose of this analysis is to learn how to utilize machine learning to solve a very specific problem - identifying individuals at risk of diabetes.\n",
    "\n",
    "## Contents\n",
    "1. Import dataset\n",
    "2. Data exploration\n",
    "3. Feature engineering\n",
    "4. Modeling\n",
    "\n",
    "## How to use this notebook\n",
    "- To execute any single block of text or markdown, use ctrl+enter, shift+enter or press the run arrow on the left of the box (only in Colaboratory)\n",
    "- To reset the notebook select \"Factory reset runtime\" from the Runetime tab at the top of Colaboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's import our data\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/jzhangab/DS101/master/1_Data/diabetes.csv'\n",
    "df = pd.read_csv(url, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first 5 rows to begin understanding what factors are available\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data frame we can see that the data consists of 8 different factors that contribute to the risk of diabetes. The actual truth of whether or not an individual has diabetes is in the column \"Outcome\". We will use this information to train a machine learning model to understand how the different factors are connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "The purpose of data exploration is to seek to understand the data. We will look primarily at histograms and scatterplots to visualize if there are any interesting relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the histograms of the dataframe to understand each factor.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "ax = fig.gca()\n",
    "df.hist(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age is skewed young, but how does it relate to one of the more normally distributed factors such as BloodPressure?\n",
    "# Try changing the x and y variables in the scat4terplot to view other relationships.\n",
    "%matplotlib inline\n",
    "df.plot.scatter(x = 'Age',\n",
    "                y = 'BloodPressure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about BMI vs. Outcome? Are higher BMI persons more likely to have diabetes?\n",
    "# This shows that you may be tempted to draw conclusions using single factor analysis that BMI causes diabetes - when it is only one of several factors\n",
    "%matplotlib inline\n",
    "df.plot.scatter(x = 'Outcome',\n",
    "                y = 'BMI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "The purpose of feature engineering is to prepare data for modeling. The diabetes data set is formatted well and does not contain text variables so we will only do two things to prepare the data\n",
    "\n",
    "1. Missing data\n",
    "2. Reduce multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By far the most important thing to understand about a dataset is how \"clean\" it might be\n",
    "# For cleanliness, missing data is very important, let's check how much missing data there is for each factor\n",
    "for col in list(df):\n",
    "    num_na = len(df[col]) - df[col].count()\n",
    "    print (\"Percent null in column \" + col + \" is:\", 100*num_na/len(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some columns such as Glucose, BloodPressure, SkinThickness, Insulin, BMI, and Age we are not only concerned with null values but also 0 values\n",
    "for col in list(df):\n",
    "    num_0 = len(df.loc[df[col] == 0][col])\n",
    "    print (\"Percent 0 in column \" + col + \" is:\", 100*num_0/len(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the following to clean our data\n",
    "# 1. Replace all null values with 0\n",
    "# 2. Remove any data point where any of the following are 0: Glucose, BloodPressure, SkinThickness, Insulin, BMI, or Age\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "nonzero_factors = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\n",
    "for col in nonzero_factors:\n",
    "    df = df.loc[df[col] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity\n",
    "The idea of collinearity is that if certain input factors are closely correlated, they will bias the output of the model by amplifying their particular effects. We need to understand if some of our factors are high collinear and then reduce bias by removing all but 1 of the collinear factors from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check the correlation (R-square) between variables using a correlation matrix\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To quantify multicollinearity, we will use variance inflation factor (VIF)\n",
    "# Rule of thumb, VIF above 10 indicates a particular variable ought to be removed\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "df_c = add_constant(df)\n",
    "pd.Series([variance_inflation_factor(df_c.values, i) \n",
    "               for i in range(df_c.shape[1])], \n",
    "              index=df_c.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Modeling\n",
    "In the modeling step we will train a supervised machine learning model to understand relationships in the diabetes data set. We will then evaluate the model to see how well it predicts.\n",
    "\n",
    "The particular model that we will use is Logistic Regression. This model is commonly used in binary classification for predictive analytics.\n",
    "\n",
    "1. Split dataset into training and validation datasets\n",
    "2. Train model\n",
    "3. Predict outcomes of validation dataset\n",
    "4. Calculate accuracy of validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split the data 80%/20% using 80% of the data to train the model and 20% to validate the accuracy of the model\n",
    "# We can use pre-built functions from the machine learning package sci-kit learn to do this task\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "features = [col for col in list(df) if col != 'Outcome']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[features], df['Outcome'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare and fit model\n",
    "model = LogisticRegression(random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare y_test (true values) to y_pred (predicted values)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the confusion matrix, which shows us false positives and false negatives\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
