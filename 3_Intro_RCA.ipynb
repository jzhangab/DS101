{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to Applied Machine Learning\n",
    "## Operations example based on real-world application\n",
    "In this notebook you will implement a machine learning algorithm to model a complex system that has been anonymized from a real problem within the Operations space. You will understand how to look beyond single factor analysis and utilize machine learning to model the system and also discover key factors that contribute to failure modes.\n",
    "\n",
    "The data has been anonymized, randomized, and any identifying information removed. However, it is still heavily derived from types of operations data you may have seen before and is a great way to begin thinking about how you can use data mining methods and machine learning to solve problems in your space.\n",
    "\n",
    "## Contents\n",
    "1. Import dataset\n",
    "2. Data exploration\n",
    "3. Feature engineering\n",
    "4. Modeling\n",
    "5. Model Evaluation\n",
    "\n",
    "## How to use this notebook\n",
    "- To execute any single block of text or markdown, use ctrl+enter, shift+enter or press the run arrow on the left of the box (only in Colaboratory)\n",
    "- To reset the notebook select \"Factory reset runtime\" from the Runtime tab at the top of Colaboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's import our data. The data is split into 2 files\n",
    "# data_url contains manufacturing-related data that describes the conditions in which the product was manufactured\n",
    "# label_url contains information related to the outcome of each product subset - and we will discuss later what constitutes bad vs. good product\n",
    "import pandas as pd\n",
    "\n",
    "data_url = 'https://raw.githubusercontent.com/jzhangab/DS101/master/1_Data/data.csv'\n",
    "label_url = 'https://raw.githubusercontent.com/jzhangab/DS101/master/1_Data/outcome.csv'\n",
    "df = pd.read_csv(data_url, sep = ',')\n",
    "df_label = pd.read_csv(label_url, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the first 5 rows to begin understanding what factors are available\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at the outcome dataset\n",
    "df_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can begin thinking/answering some questions you may have about this data.\n",
    "\n",
    "1. How is each data point separated? What denotes a unique data point?\n",
    "2. How many data points do we have?\n",
    "3. Do we have any missing data?\n",
    "4. Do we only have numerical data?\n",
    "5. How many features do we potentially have?\n",
    "6. What is the question we are trying to answer with this data, can this data answer this question?\n",
    "7. etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2. Data Exploration\n",
    "Let's now begin exploring the data we have to help ourselves understand the problem and see what approach we might want to take.\n",
    "\n",
    "Before we can begin plotting our data, we have to deal with 2 problems that we encounter in this dataset that we did not encounter in the Pima Indian Diabetes data set.\n",
    "\n",
    "1. There is a value \"Low Value\" that is a replacement for undetectable signal. We have to replace this with a number.\n",
    "2. There are null values in our parameter columns that we need to fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleanliness, missing data is very important, let's check how much missing data there is for each factor\n",
    "for col in list(df):\n",
    "    num_na = len(df[col]) - df[col].count()\n",
    "    print (\"Percent null in column \" + col + \" is:\", 100*num_na/len(df[col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's check the number of \"Low Value\" in each column\n",
    "for col in list(df):\n",
    "    num_lv = len(df.loc[df[col] == 'Low Value'][col])\n",
    "    print (\"Percent Low Value in column \" + col + \" is:\", 100*num_lv/len(df[col]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important observations\n",
    "1. Thanks domain knowledge, we know that \"Low Value\" is a placeholder for any measurement that is less than 0.5. Therefore we will replace this by the average of values 0-0.5 (0.25)\n",
    "2. A-priori we don't care about the DATE of manufacture because we know that our process should be in control regardless of the date so we will not consider it for this analysis. This does not mean that in your own analysis on a separate problem that DATE will not be important.\n",
    "3. We also do not care for the DESCRIPTOR_1 and SOURCE columns because we know they are unrelated to manufacturing process. (Again, for just this particular problem. For your own work unstructured data may apply.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do the following to clean our data\n",
    "# 1. Replace all null values with 0\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# 2. Replace Low Value with 0.25\n",
    "df = df.replace(\"Low Value\", 0.25)\n",
    "\n",
    "# 3. Convert parameter columns to float64 from string objects, we have to do this because \"Low Value\" defaults each column to string objects, but the machine learning model expects numerical data\n",
    "for c in list(df):\n",
    "    if \"PARAM\" in c:\n",
    "        df[c] = df[c].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use dtypes to check the type of each column to make sure our parameters are now numerical\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's remove the non-parametric columns from our dataframe. These are columns we will not use for modeling.\n",
    "\n",
    "# This notation is a generator, a one-line for loop in python. It is used to make the code cleaner and use fewer new variable declarations.\n",
    "df = df[[c for c in list(df) if c not in ['SOURCE', 'DATE', 'DESCRIPTOR_1']]]\n",
    "\n",
    "# Alternatively it can take the form of a multi-line for loop as shown in the commented block below.\n",
    "'''\n",
    "column_list = []\n",
    "for c in list(df):\n",
    "    if c not in ['SOURCE', 'DATE', 'DESCRIPTOR_1']:\n",
    "        column_list.append(c)\n",
    "df = df[column_list]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important observation\n",
    "Another potential issue for us is determining what to do with duplicate data. We know based on domain knowledge that each data point should be unique by the NAME column, but how do we handle this if each NAME has multiple SUB_NAMES? This is very common when we deal with products that have multiple measurements for the same attribute. We need to decide if we should treat each measurement separately or aggregate them somehow using mean, max, median, or another measure of center or extremes.\n",
    "\n",
    "For this particular problem we will use the aggregation by mean. We need to do the following.\n",
    "\n",
    "1. Figure out if there are duplicate entires by the NAME column\n",
    "2. If there are, we need to average all data by SUB_NAME for each NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there duplicate names? We can check for this by doing a value count for the NAME column\n",
    "# The function value_counts allows us to count the number of each unique value in the column NAME\n",
    "df['NAME'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like there are some NAMEs with up to 32 SUB-NAMEs, we need to average the PARAM values for each NAME\n",
    "# The groupby function aggregates columns of the dataframe using a list of index columns. Here we want to aggregate by NAME only so we get mean of each parameter by NAME\n",
    "df = df.groupby(['NAME']).mean().reset_index()\n",
    "\n",
    "# Remove SUB_NAME from columns because we no longer care about the SUB_NAMEs after averaging by NAME\n",
    "df = df[[c for c in list(df) if c not in ['SUB_NAME']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the histograms of the dataframe to understand each factor.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (15,15))\n",
    "ax = fig.gca()\n",
    "df.hist(ax = ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "That's a lot of histograms! We can make some quick observations.\n",
    "\n",
    "1. Some of the data is normally distributed (PARAM_1, PARAM_10, PARAM_2, PARAM_8)\n",
    "2. Some of the data is skewed (PARAM_3, PARAM_4)\n",
    "3. Some of the data is multi-modal (PARAM_5, PARAM_6, PARAM_7, PARAM_9)\n",
    "4. Some of the parameters are between 0 and 1 while others are between 0 and 10, is this going to be a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The non-normally distributed data is very interesting, perhaps they are related to each other?\n",
    "%matplotlib inline\n",
    "df.plot.scatter(x = 'PARAM_5',\n",
    "                y = 'PARAM_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using what you have learned about plots, try plotting some of the columns using histograms, scatterplots, or regression models to explore the data\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. Feature Engineering\n",
    "We've already performed a lot of data cleaning in the exploration phase. Now we need to finish the job by combining the feature and label dataframes as well as creating our classification label\n",
    "\n",
    "1. Merge feature data and label data\n",
    "2. Decide which features to use\n",
    "3. Create a label (for binary classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The labels are located in the dataframe df_label\n",
    "# df_label has the same issue as the parameter dataframe in that there are NAMEs with duplicates by SUB_NAME\n",
    "df_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there are duplicate NAME using value_counts\n",
    "df_label['NAME'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's aggregate on NAME using mean\n",
    "df_label = df_label.groupby(['NAME']).mean().reset_index()\n",
    "\n",
    "# Remove SUB_NAME from columns because we no longer care about the SUB_NAMEs after averaging by NAME\n",
    "df_label = df_label[[c for c in list(df_label) if c != 'SUB_NAME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important observation\n",
    "How do we determine if a particular data point represents bad product? Well thanks to a-priori knowledge we know that there is an existing process-monitoring and controls action limit at VALUE = 2.0. Let's explore the label using this action limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's plot a histogram to visualize the label measure distribution\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize = (5, 5))\n",
    "ax = fig.gca()\n",
    "df_label.hist(ax = ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmm, by the way how many standard deviations from the mean is 2.0?\n",
    "\n",
    "# First calculate the standard deviation of a column\n",
    "sd = df_label['MEASURE'].std()\n",
    "\n",
    "# Then calculate the mean\n",
    "mean = df_label['MEASURE'].mean()\n",
    "\n",
    "# Finally print # SD between mean and action limit (2.0)\n",
    "print(\"Action limit # SD from mean:\", (2-mean)/sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Important observation\n",
    "\n",
    "1. The label measure itself appears to be normally distributed.\n",
    "2. The action limit is 2.85 SD from mean which means our system does not quite meet the 3.0 SD threshold normally found in most preventive capability analysis systems.\n",
    "3. We have an imbalanced dataset, there are far more \"good\" datapoints than \"bad\" datapoints, as bad datapoints are only those above the action limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a classification label, we need to convert the continuous column MEASURE into binary column (0 or 1) LABEL\n",
    "\n",
    "# First create a new column and set all values to 0 (not a failure)\n",
    "df_label['LABEL'] = 0\n",
    "\n",
    "# Set data point values in the new column LABEL to 1 that is greater than or equal to the action limit (2.0) in the continuous column MEASURE\n",
    "df_label.loc[df_label['MEASURE'] >= 2, 'LABEL'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multicollinearity\n",
    "The idea of collinearity is that if certain input factors are closely correlated, they will bias the output of the model by amplifying their particular effects. We need to understand if some of our factors are highly collinear and then reduce bias by removing all but 1 of the collinear factors from the dataframe.\n",
    "\n",
    "1. Let's check if we have a collinearity problem in our parameter dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can check the correlation (R-square) between variables using a correlation matrix\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To quantify multicollinearity, we will use variance inflation factor (VIF)\n",
    "# Rule of thumb, VIF above 10 indicates a particular variable ought to be removed\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "# Also - VIF for a constant term should be high because the intercept is a proxy for the constant.\n",
    "# A constant term needs to be added to accurately measure VIF for the other terms\n",
    "df_c = add_constant(df[[c for c in list(df) if 'PARAM' in c]])\n",
    "\n",
    "# inline Generator on a pandas series\n",
    "pd.Series([variance_inflation_factor(df_c.values, i) \n",
    "               for i in range(df_c.shape[1])], \n",
    "              index=df_c.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lastly we have to merge the feature and label dataframes for the modeling phase of the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left merge label onto features (look up SQL left join if unsure what this does)\n",
    "df = pd.merge(df, df_label, on = 'NAME', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4. Modeling\n",
    "In the modeling step we will train a supervised machine learning model to understand relationships in the operations data set. We will then evaluate the model to see how well it predicts.\n",
    "\n",
    "The particular model that we will use is the random forest algorithm, a classical machine learning algorithm very useful for classification tasks.\n",
    "\n",
    "1. Split dataset into training and validation datasets\n",
    "2. Train model\n",
    "3. Predict outcomes of validation dataset\n",
    "4. Calculate accuracy of validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will split the data 80%/20% using 80% of the data to train the model and 20% to validate the accuracy of the model\n",
    "# We can use pre-built functions from the machine learning package sci-kit learn to do this task\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# The NAME, MEASURE, and LABEL columns are not part of input features so we will use a generator to create a new list and call it \"features\"\n",
    "features = [col for col in list(df) if col not in ['NAME', 'MEASURE', 'LABEL']]\n",
    "\n",
    "# The X input is df[features] which is all columns in the dataframe of the list features we created using the generator\n",
    "# The Y input is df['LABEL'] which is the binary label column we created in data exploration\n",
    "X = df[features]\n",
    "y = df['LABEL']\n",
    "\n",
    "# Generate the 4 datasets we need\n",
    "# X_train and y_train to train the model\n",
    "# X_test to generate predictions\n",
    "# y_test to evaluate the accuracy of the predictions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare and fit model\n",
    "# We have to use the hyperparameter class_weight because of the very imbalanced classes (very few 1's compared to 0's)\n",
    "# Balanced class weight uses the inverse of frequency to weight each class\n",
    "model = RandomForestClassifier(random_state=0, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using test set. These are binary predictions of 1's and 0's\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5. Model Evaluation\n",
    "We will use several techniques to evaluate the strength of the Model\n",
    "\n",
    "1. Accuracy\n",
    "2. Confusion Matrix (false positive, true positive, false negative, true negative)\n",
    "3. Receiver operating characteristic\n",
    "4. Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare y_test (true values) to y_pred (predicted values)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the confusion matrix, which shows us false positives and false negatives\n",
    "# According to the confusion matrix we have only a single false negative and selected correctly the other 2 positive classes\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method of evaluating a classifier is using the Receiver Operating Characteristic (ROC)\n",
    "# ROC is a plot of true positive vs. false positive. We calculate the area under the curve (AUC)\n",
    "# AUC = 1 indicates a perfect classifier, AUC = 0.5 means the classifier is no better than a coin flip\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "%matplotlib inline\n",
    "y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "falseposrate, trueposrate, _ = roc_curve(y_test, y_pred_proba)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "plt.plot(falseposrate,trueposrate,label=\"ROC curve, auc=\"+str(auc))\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# With tree models we can use feature importance to get some insight into root cause\n",
    "\n",
    "# Mean feature importance across all trees\n",
    "mean_i = model.feature_importances_\n",
    "\n",
    "# Standard deviation of feature importances across all trees\n",
    "st_i = np.std([tree.feature_importances_ for tree in model.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "# Features\n",
    "features_i = np.argsort(mean_i)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X_train.shape[1]):\n",
    "    print(\"%d. PARAM %d (%f)\" % (f + 1, features_i[f]+1, mean_i[features_i[f]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preliminary Conclusions\n",
    "It appears we can make some preliminary analysis about the root cause of the products that fall above the action limit. Next steps for such an analysis would be to investigate the physical relationship between PARAM_8 and the manufacturing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
